* [[https://blog.csdn.net/fwj_ntu/article/details/78843872][使用Beautifulsoup解析本地html文件_fwj_ntu 的博客-CSDN博客]] :website:

[2020-09-24 四 20:15]

** Article

有时候网络上的资源可以通过 html 保存在本地，然后再解析本地 html 文件，提取有用的信息。基本的实现思路如下：

*1. 打开 html 文件*\\
比如，待解析的 html 文件保存在 d 盘。这里插一句，要读一个东西，首先得知道这东西在哪里。如果它在万维网上，那么我们需要知道 URL。

URL(Uniform Resoure Locator)，统一资源定位符。是对可以从互联网上得到的资源的位置和访问方法的一种简洁的表示，是互联网上标准资源的地址。它是一个网络路径。

本地路径，与网络路径相对应。又称物理路径，指的是某一台计算机本地的路径，以盘符开头，例如 C:\、D:\temp 等等。

此处，我们要想打开本地 html 文件，首先得告诉程序你的文件在哪里。

#+BEGIN_EXAMPLE
     path =  'd:/Download/mydoc.html'
#+END_EXAMPLE

然后，使用 open 函数打开文件。

#+BEGIN_EXAMPLE
     htmlfile =  open(path,  'r', encoding= 'utf-8')
#+END_EXAMPLE

*2. 读取 html 的句柄内容*\\
这里所说的“句柄内容”就是指 html 文件里面的内容。为什么要这么做？

谨记，Beautifulsoup 的第一个参数是一个 html 句柄内容，而不是 html 文件。所以我们要把 html 文件里的内容读出来，然后传递给它。

#+BEGIN_EXAMPLE
      htmlhandle =  htmlfile.read()
#+END_EXAMPLE

*3. 使用 Beautifulsoup 解析*\\
调用 Beautifulsoup 解析功能，解析器使用 lxml。

#+BEGIN_EXAMPLE
      from bs4  import Beautifulsoup

    soup = Beautifulsoup(htmlhandle,  'lxml')
#+END_EXAMPLE

*4. 抽取有用的字段*\\
为了便于后期的数据分析，小量数据，数据分析师最爱用 Excel。所以，此处我们利用 python 中 pandas 包的 dataframe 数据框架，来存储 html 文件中目标字段的值。

这次，尝试了使用以下这种方式，向 dataframe 追加新的行。

#+BEGIN_EXAMPLE
     import pandas  as pd

    count =  0
     result = pd.DataFrame({},index=[ 0])
     result[ 'author'] =  ''
     result[ 'title'] =  ''
     result[ 'source'] =  ''
     new =  result
     for  item  in soup.find_all( 'tr'):
         if  'AU '  in  item.get_text():
            author =  item.get_text()
             new[ 'author'] = author
        elif  'TI '  in  item.get_text():
            title =  item.get_text()
             new[ 'title'] = title
        elif  'SO '  in  item.get_text():
            source =  item.get_text()
             new[ 'source'] = source
            count +=  1
             result =  result.append( new,ignore_index=True)
    print(count)
#+END_EXAMPLE

最后，将 dataframe 中的数据保存到 excel 中去，文件写入真是贼方便。给 python 打 call，6666666.....

#+BEGIN_EXAMPLE
      result.to_excel( 'd:result.xlsx')
#+END_EXAMPLE
